---
title: "Part III. Topic Modeling and Performance Evaluation"
author: "Aishat Sadiq"
date: '2024-02-27'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Left ToDo:
- Select raw df
- Update code to reflect correct df and variables
- Send to JG for Github Upload


Data Preprocessing & Feature Selection

Load Libraries
```{r Load Libraries}
# install.packages() 
library(tidyverse)
library(tidytext)
library(topicmodels)
library(superheat)
library(ggrepel)
library(wordcloud)
# library(corrplot)     # maybe delete, can prob use gg instead
# library(Rdatasets)    #  collection of 2264 datasets
```

Load & Examine Dataset
```{r Load & Examine Dataset}

devtools::install_github("ccss-rs/NAMEOFREPO")
set.seed(31415)

# Download from github: https://github.com/ccss-rs
getwd()
list.files()
data <- read_csv("/Users/aishatsadiq/Library/Mobile Documents/iCloud~md~obsidian/Documents/PhD/CCSS Data Fellow/labor_market_discrimination.csv")

# AssociatedPress dataset is a collection of 2246 news articles from an American news agency, mostly published around 1988.
library(topicmodels)
data("AssociatedPress")
AssociatedPress

# create a new column story that keeps track of which of the twelve short stories each line of text comes from, and remove the preliminary material that comes before the first story actually starts.
sherlock <- books %>%
    mutate(story = ifelse(str_detect(text, "ADVENTURE"),
                          text,
                          NA)) %>%
    fill(story) %>%
    filter(story != "THE ADVENTURES OF SHERLOCK HOLMES") %>%
    mutate(story = factor(story, levels = unique(story)))


```

Tidy Data & Tokenization
1. Split raw text into "tokens"
2. Remove stop words, 'casing, stemming
3. Calculate word count

Across these 4 books, we have 65 chapters and a vocabulary of size 18325.
```{r Tokenization}

# split raw text to individual words/tokens using token = " "
word_counts <- by_chapter %>%
  tidytext::unnest_tokens(word, text, token = "words", to_lower = TRUE) %>%   #Lowercasing
  anti_join(stop_words) %>%
  filter(word != "holmes")  %>% # domain-expert feature selection decision: remove the word “holmes” because it is so common and used neutrally in all twelve stories
  count(document, word, sort=TRUE)  # calculate word count

# Lowercasing w/ base R's tolower(), could also use toupper(x) for uppercase transformation
df[c('team', 'conf')] <- sapply(df[c('team', 'conf')], function(x) tolower(x))

###### Making long region data wide w/ pivot_wider()
ncol(Preproc_OutIQR)
Preproc_wide <- 
  Preproc_OutIQR  %>% 
  tidyr::pivot_wider(names_from = "region", 
                     values_from = "participants")
ncol(Preproc_wide)

###### convert data from long to wide 
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

```

tf-idf statistic
The statistic tf-idf identifies words that are important to a document in a collection of documents; in this case, we’ll see which words are important in one of the stories compared to the others.
```{r}
sherlock_tf_idf <- tidy_sherlock %>%
    count(story, word, sort = TRUE) %>%
    bind_tf_idf(word, story, n) %>%
    arrange(-tf_idf) %>%
    group_by(story) %>%
    top_n(10) %>%
    ungroup

sherlock_tf_idf %>%
    mutate(word = reorder_within(word, tf_idf, story)) %>%
    ggplot(aes(word, tf_idf, fill = story)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ story, scales = "free", ncol = 3) +
    scale_x_reordered() +
    coord_flip() +
    theme(strip.text=element_text(size=11)) +
    labs(x = NULL, y = "tf-idf",
         title = "Highest tf-idf words in Sherlock Holmes short stories",
         subtitle = "Individual stories focus on different characters and narrative elements")
```


Feature Extraction

Lexicon Approach to Sentiment analysis
```{r Lexicon Approach}

# most common bigrams
df %>%
  dplr::count(bigram, sort = TRUE)

# View package and function inputs
?tidytext::get_sentiments()

# Download established sentiment lexicon from Saif Mohammad and Peter Turney
get_sentiments("nrc")  

# Run analysis w/ selected sentiment lexicon/dictionary
jane_austen_sentiment <- tidy_books %>%
  inner_joinget_sentiments("nrc")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%     # index keeps track of narrative time in sections of text
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# plot these sentiment scores across the plot trajectory of each novel
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```


Latent Dirichlet Allocation (LDA)

```{r LDA}
?LDA()
# practice first setting k = 2 to create a two-topic LDA model
lda <- LDA(data, k = 6)
lda

# tranform df from DocumentTermMatrix to table with one token per row
chapters_dtm <- word_counts %>%
  tidytext::cast_dtm(document, word, n)

# create a four-topic model based on # chapters/domain knowledge
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))

# show the most closely related terms for each of the topics
as.data.frame(terms(chapters_lda, 4))

# show the topic with the highest proportion for each document
data.frame(Topic = topics(lda.model))

# Extract LDA model β, the per-topic-per-word probabilities
tidy(chapters_lda, matrix = "beta")

# examine the per-document-per-topic probabilities, called γ (“gamma”)
ap_documents <- tidy(ap_lda, matrix = "gamma")

assignments <- augment(chapters_lda, data = chapters_dtm)
assignments <- assignments %>%
  separate(document, c("title", "chapter"), 
           sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))




```

```{r dtm method for lda model}
#create DTM
dtm <- CreateDtm(tokens$text, 
                 doc_names = tokens$ID, 
                 ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm

k_list <- seq(1, 20, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
  filename = file.path(model_dir, paste0(k, "_topics.rda"))
  
  if (!file.exists(filename)) {
    m <- FitLdaModel(dtm = dtm, k = k, iterations = 500)
    m$k <- k
    m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
    save(m, file = filename)
  } else {
    load(filename)
  }
  
  m
}, export=c("dtm", "model_dir")) # export only needed for Windows machines
#model tuning
#choosing the best model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)), 
                            coherence = sapply(model_list, function(x) mean(x$coherence)), 
                            stringsAsFactors = FALSE)
ggplot(coherence_mat, aes(x = k, y = coherence)) +
  geom_point() +
  geom_line(group = 1)+
  ggtitle("Best Topic by Coherence Score") + theme_minimal() +
  scale_x_continuous(breaks = seq(1,20,1)) + ylab("Coherence")

```


Interpretation
*term frequency (tf) & Word-topic probabilities
*inverse document frequency (idf)

turns the model into a one-topic-per-term-per-row format. For each combination of term-topic, the model computes the probability of that term being generated from that topic. 
```{r Interpretation}

# most common words in specific document
tidy(AssociatedPress) %>%
  filter(document == 6) %>%
  arrange(desc(count))

# find the 10 terms that are most common within each topic
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# viz - barplots most common terms
ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

Visualization

visualization lets us understand the topics that were extracted from the text
```{r Visualization}
# Wordclouds w/ wordcloud()
# common words in Jane Austen’s works as a whole
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

# creates wordcloud determine the most common positive and negative words
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
# highest weight words per topic
topics %>%
  arrange(topic, -beta)
# topic memberships per document
memberships %>%
  arrange(document, topic)
save(topics, memberships, file = "12-1.rda")

# construct a faceted barplot w/ arbitrary threshold of probability>0.0003
ggplot(topics %>% filter(beta > 3e-4), aes(term, beta)) +
  geom_col() +
  facet_grid(topic ~ .) +
  theme(axis.text.x = element_blank())

# construct a heatmap
topics %>%
  filter(beta > 3e-4) %>%
  pivot_wider(names_from = "term", values_from = "beta", values_fill = 0, names_repair = "unique") %>%
  select(-1) %>%
  superheat(
  pretty.order.cols = TRUE,
  legend = FALSE
  )

# aggregate topic proportions for each chapter
memberships <- memberships %>%
  mutate(
book = str_extract(document, "[^_]+"),
topic = factor(topic)
  )

ggplot(memberships, aes(topic, gamma)) +
  geom_boxplot() +
  facet_wrap(~book)


```

Code Sources
*https://krisrs1128.github.io/stat679_notes/2022/06/02/week12-1.html
*https://krisrs1128.github.io/stat679_notes/2022/06/02/week12-2.html
*https://juliasilge.com/blog/sherlock-holmes-stm/
*https://www.tidytextmining.com/topicmodeling

